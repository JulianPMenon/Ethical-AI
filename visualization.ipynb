{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "plot_dir = Path(\"plots\")\n",
    "if not plot_dir.exists():\n",
    "    plot_dir.mkdir()\n",
    "\n",
    "experiment1_f = Path(\"training_logs/deberta-v3-large/trainer_state.json\")\n",
    "experiment2_f = Path(\"training_logs/multimodel_finetuning_deberta-v3-base/checkpoint-10220/trainer_state.json\")\n",
    "\n",
    "experiment1_val_f = Path(\"results/deberta-v3-large/validation.csv\")\n",
    "experiment1_test_f = Path(\"results/deberta-v3-large/test.csv\")\n",
    "experiment2_test_f = Path(\"results/multimodel_finetuning_deberta-v3-base/test.csv\")\n",
    "\n",
    "baseline_val_f = Path(\"results/baseline/validation.csv\")\n",
    "baseline_test_f = Path(\"results/baseline/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileloader\n",
    "baseline_val_df = pd.read_csv(baseline_val_f, index_col=0)\n",
    "baseline_test_df = pd.read_csv(baseline_test_f, index_col=0)\n",
    "\n",
    "experiment1_val_df = pd.read_csv(experiment1_val_f, index_col=0)\n",
    "experiment1_test_df = pd.read_csv(experiment1_test_f, index_col=0)\n",
    "experiment2_test_df = pd.read_csv(experiment2_test_f, index_col=0)\n",
    "\n",
    "def load_experiment(experiment_file) -> pd.DataFrame:\n",
    "    with open(experiment_file, \"r\") as file:\n",
    "        experiment_df = pd.DataFrame(json.load(file)[\"log_history\"]).set_index(\"epoch\")\n",
    "        # contains 2 entries per epoch\n",
    "        extra_cols = [\"grad_norm\", \"learning_rate\", \"loss\", \"step\"]\n",
    "        train_df = experiment_df[extra_cols].dropna()\n",
    "        experiment_df = experiment_df[experiment_df[\"loss\"].isna()].drop([\"grad_norm\", \"learning_rate\", \"loss\", \"step\"], axis=1)\n",
    "        #experiment_df = pd.concat([train_df, experiment_df], axis=1)\n",
    "        return experiment_df\n",
    "\n",
    "experiment1_df = load_experiment(experiment1_f)\n",
    "experiment2_df = load_experiment(experiment2_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display dataframes\n",
    "display(experiment1_df)\n",
    "display(baseline_val_df)\n",
    "display(baseline_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(metric: str, experiments: list[pd.DataFrame], experiment_names: list[str], prefixes: list[str] | str = \"eval_\") -> tuple[pd.DataFrame, str]:\n",
    "    parsed_metric = metric.replace(\"_\", \" \").title()\n",
    "    if isinstance(prefixes, list):\n",
    "        for i, prefix, name, exp in zip(range(len(experiments)), prefixes, experiment_names, experiments):\n",
    "            cur_metric = f\"{prefix}{metric}\"\n",
    "            experiments[i] = exp.rename({cur_metric: parsed_metric}, axis=1)\n",
    "            experiments[i][\"Experiment\"] = name.replace(\"_\", \" \").title()\n",
    "\n",
    "    else:\n",
    "        for i, name, exp in zip(range(len(experiments)), experiment_names, experiments):\n",
    "            cur_metric = f\"{prefixes}{metric}\"\n",
    "            experiments[i] = exp.rename({cur_metric: parsed_metric}, axis=1)\n",
    "            experiments[i][\"Experiment\"] = name.replace(\"_\", \" \").title()\n",
    "    \n",
    "    formatted_df = pd.concat(experiments, axis=0)\n",
    "    formatted_df.index.rename(\"Epoch\", inplace=True)\n",
    "    return formatted_df, parsed_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot creation\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "BASELINE_COLOR = \"black\"\n",
    "CONTEXT = \"poster\"\n",
    "FIGSIZE = (11, 7)\n",
    "\n",
    "def plot_training_progress(metric: str, experiments: list[pd.DataFrame], experiment_names: list[str], prefixes: list[str] | str = \"eval_\", baseline: pd.DataFrame = None):\n",
    "    formatted_df, metric_name = format_df(metric, experiments, experiment_names, prefixes)\n",
    "    formatted_df = formatted_df[[metric_name, \"Experiment\"]]\n",
    "    \n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "\n",
    "    seaborn.lineplot(formatted_df, x=formatted_df.index, y=metric_name, hue=\"Experiment\")\n",
    "\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "    if not baseline is None:\n",
    "        plt.hlines(baseline[f\"validation_{metric}\"].iloc[0], xmin=0, xmax=max(formatted_df.index), colors=[BASELINE_COLOR])\n",
    "        patch = matplotlib.patches.Patch(color=BASELINE_COLOR, linewidth=1.0, label='Baseline')\n",
    "        handles.append(patch) \n",
    "    \n",
    "    plt.legend(handles=handles, title=\"Experiment\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(\"Training Progress\")\n",
    "    seaborn.set_theme(context=CONTEXT, palette=\"pastel\")\n",
    "    seaborn.despine(top=True, left=True, bottom=True, right=True)\n",
    "    seaborn.set_style(\"whitegrid\", rc={\"c\": (0.95, 0.95, 0.95)})\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_metrics(metrics: list[str], experiment: pd.DataFrame, experiment_name: str, prefix: str = \"eval_\"):\n",
    "    formatted_df = pd.DataFrame(columns=[\"Value\", \"Metric\"])\n",
    "    for metric in metrics:\n",
    "        df, metric_name = format_df(metric, [experiment], [metric], prefix)\n",
    "        formatted_df = pd.concat([formatted_df, df.rename({metric_name: \"Value\", \"Experiment\": \"Metric\"}, axis=1)[[\"Value\", \"Metric\"]]])\n",
    "    formatted_df.index.rename(\"Epoch\", inplace=True)\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "\n",
    "    seaborn.lineplot(formatted_df, x=formatted_df.index, y=\"Value\", hue=\"Metric\")\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    plt.title(experiment_name)\n",
    "    seaborn.set_theme(context=CONTEXT, palette=\"pastel\")\n",
    "    seaborn.despine(top=True, left=True, bottom=True, right=True)\n",
    "    seaborn.set_style(\"whitegrid\", rc={\"c\": (0.95, 0.95, 0.95)})\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_result_metrics(metrics: list[str], experiments: list[pd.DataFrame], experiment_names: list[str], prefixes: list[str] | str = \"eval_\"):\n",
    "    formatted_df = pd.DataFrame(columns=[\"Value\", \"Metric\", \"Experiment\"])\n",
    "    for metric in metrics:\n",
    "        df, metric_name = format_df(metric, experiments, experiment_names, prefixes)\n",
    "        df[\"Metric\"] = metric_name\n",
    "        formatted_df = pd.concat([formatted_df, df.rename({metric_name: \"Value\"}, axis=1)[[\"Value\", \"Metric\", \"Experiment\"]]])\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "\n",
    "    graph = seaborn.catplot(formatted_df, x=\"Experiment\", y=\"Value\", col=\"Metric\", hue=\"Experiment\", kind=\"bar\")\n",
    "\n",
    "    graph.set_xticklabels(rotation=45)\n",
    "    graph.set_titles(\"{col_name}\")\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    #plt.suptitle(\"Results\")\n",
    "    seaborn.set_theme(context=CONTEXT, palette=\"pastel\")\n",
    "    seaborn.despine(top=True, left=True, bottom=True, right=True)\n",
    "    seaborn.set_style(\"whitegrid\", rc={\"c\": (0.95, 0.95, 0.95)})\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_result_metrics_single(categories: list[str], experiment: pd.DataFrame, experiment_name: str, prefix: str = \"eval_\", y_label: str = \"Value\"):\n",
    "    y_label = y_label.title()\n",
    "    formatted_df = pd.DataFrame(columns=[y_label, \"Class\", \"Experiment\"])\n",
    "    for category in categories:\n",
    "        df, metric_name = format_df(category, [experiment], [experiment_name], prefix)\n",
    "        df[\"Class\"] = metric_name\n",
    "        formatted_df = pd.concat([formatted_df, df.rename({metric_name: y_label}, axis=1)[[y_label, \"Class\", \"Experiment\"]]])\n",
    "\n",
    "    plt.figure(figsize=FIGSIZE)\n",
    "\n",
    "    ax = seaborn.barplot(formatted_df, x=\"Class\", y=y_label, hue=\"Class\")\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    #graph.set_titles(\"{col_name}\")\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    #plt.suptitle(\"Results\")\n",
    "    seaborn.set_theme(context=CONTEXT, palette=\"pastel\")\n",
    "    seaborn.despine(top=True, left=True, bottom=True, right=True)\n",
    "    seaborn.set_style(\"whitegrid\", rc={\"c\": (0.95, 0.95, 0.95)})\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_name = \"Deberta-v3-large\"\n",
    "exp2_name = \"Multi-Binary-Deberta-v3-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"f1_macro\"\n",
    "experiment2_df = experiment2_df.iloc[:10]\n",
    "experiments = [experiment1_df, experiment2_df]\n",
    "experiment_names = [exp1_name, exp2_name]\n",
    "prefixes = [\"eval_\", \"eval_\"]\n",
    "\n",
    "plot_training_progress(metric, experiments, experiment_names, prefixes, baseline=baseline_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"f1_macro\", \"f1_micro\", \"f1_weighted\"]\n",
    "experiment = experiment1_df\n",
    "experiment_name = exp1_name\n",
    "prefix = \"eval_\"\n",
    "\n",
    "plot_training_metrics(metrics, experiment, experiment_name, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"f1_macro\", \"precision_macro\", \"recall_macro\"]\n",
    "experiment = experiment1_df\n",
    "experiment_name = exp1_name\n",
    "prefix = \"eval_\"\n",
    "\n",
    "plot_training_metrics(metrics, experiment, experiment_name, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"f1_macro\", \"f1_micro\", \"f1_weighted\"]\n",
    "experiments = [experiment1_test_df, experiment1_val_df, baseline_test_df]\n",
    "experiment_names = [\"Deberta-v3-l Test\", \"Deberta-v3-l Val\", \"Baseline Test\"]\n",
    "prefixes = [\"test_\", \"validation_\", \"test_\"]\n",
    "\n",
    "plot_result_metrics(metrics, experiments, experiment_names, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"f1_macro\", \"f1_micro\", \"f1_weighted\"]\n",
    "experiments = [experiment1_test_df, experiment2_test_df, baseline_test_df]\n",
    "experiment_names = [\"Deberta-v3-l\", \"Multi-Binary\", \"Baseline\"]\n",
    "prefixes = [\"test_\", \"test_\", \"test_\"]\n",
    "\n",
    "plot_result_metrics(metrics, experiments, experiment_names, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_metric = \"precision\"\n",
    "classes = [ # class layer\n",
    "    'Inappropriateness', # 0\n",
    "    \n",
    "    'Toxic Emotions', # 1\n",
    "    'Excessive Intensity', # 2\n",
    "    'Emotional Deception', # 2\n",
    "    \n",
    "    'Missing Commitment', # 1\n",
    "    'Missing Seriousness', # 2\n",
    "    'Missing Openness', # 2\n",
    "    \n",
    "    'Missing Intelligibility', # 1\n",
    "    'Unclear Meaning', # 2\n",
    "    'Missing Relevance', # 2\n",
    "    'Confusing Reasoning', # 2\n",
    "    \n",
    "    'Other Reasons', # 1\n",
    "    'Detrimental Orthography', # 2\n",
    "    'Reason Unclassified', # 2\n",
    "]\n",
    "categories = [f\"{class_}\" for class_ in classes]\n",
    "experiment = experiment1_test_df\n",
    "experiment_name = \"Deberta-v3-l Test\"\n",
    "prefix = f\"test_{cls_metric}_\"\n",
    "\n",
    "plot_result_metrics_single(categories, experiment, experiment_name, prefix, cls_metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eai-24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
